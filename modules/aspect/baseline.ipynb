{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow_addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D,GRU\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_addons as tfa\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=57\n",
    "feat_vec=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../encode_data_shopee.txt\", \"r\") as f:\n",
    "    encode_data_shopee = json.loads(f.read())\n",
    "with open (\"../../encode_data_tiki.txt\", \"r\") as f:\n",
    "    encode_data_tiki = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in range(len(encode_data_shopee)):\n",
    "     if len(encode_data_shopee[i]) > max_len:\n",
    "            max_len = len(encode_data_shopee[i])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5237, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_mat = np.load(\"./weights/embedding_mat.npy\")\n",
    "embed_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_doc_shopee=pad_sequences(encode_data_shopee,maxlen=max_len,padding=\"post\")\n",
    "padded_doc_tiki=pad_sequences(encode_data_tiki,maxlen=max_len,padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../output_shopee.txt\", \"r\") as f:\n",
    "    output_ori_shopee = np.array(json.loads(f.read()))\n",
    "with open (\"../../output_tiki.txt\", \"r\") as f:\n",
    "    output_ori_tiki = np.array(json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2951, 57) (2951, 7)\n"
     ]
    }
   ],
   "source": [
    "print(padded_doc_shopee.shape, output_ori_shopee.shape)\n",
    "X_train_shopee, X_test_shopee, y_train_ori_shopee, y_test_ori_shopee = train_test_split(padded_doc_shopee, output_ori_shopee, test_size=0.2, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_shopee = np.delete(y_train_ori_shopee, 6, 1)\n",
    "y_test_shopee = np.delete(y_test_ori_shopee, 6, 1)\n",
    "# output_train_shopee = np.delete(output_ori_shopee, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.5\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = tfa.metrics.F1Score(num_classes=6, average='macro', threshold=0.5)\n",
    "# rlp = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min', patience=3, verbose=1, factor=0.4)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min', patience=6, verbose=1, restore_best_weights=True)\n",
    "filepath = ('./weights/best_teacher.h5')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1_score', verbose=1, save_best_only=True,save_weights_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U --user numpy==1.18.5\n",
    "# model = get_model(padded_doc, X_test, output_ori_pre, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 57)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 57, 300)           1571100   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 57, 256)           439296    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 55, 64)            49216     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,077,794\n",
      "Trainable params: 2,077,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3941 - f1_score: 0.6823\n",
      "Epoch 00001: val_f1_score did not improve from 0.81291\n",
      "37/37 [==============================] - 6s 175ms/step - loss: 0.3941 - f1_score: 0.6823 - val_loss: 0.2729 - val_f1_score: 0.5429\n",
      "Epoch 2/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2149 - f1_score: 0.7011\n",
      "Epoch 00002: val_f1_score did not improve from 0.81291\n",
      "37/37 [==============================] - 6s 158ms/step - loss: 0.2149 - f1_score: 0.7011 - val_loss: 0.2092 - val_f1_score: 0.7574\n",
      "Epoch 3/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1306 - f1_score: 0.8680\n",
      "Epoch 00003: val_f1_score improved from 0.81291 to 0.83365, saving model to ./weights\\best_teacher.h5\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.1306 - f1_score: 0.8680 - val_loss: 0.2040 - val_f1_score: 0.8337\n",
      "Epoch 4/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0883 - f1_score: 0.9188\n",
      "Epoch 00004: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0883 - f1_score: 0.9188 - val_loss: 0.2536 - val_f1_score: 0.8151\n",
      "Epoch 5/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0610 - f1_score: 0.9468\n",
      "Epoch 00005: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0610 - f1_score: 0.9468 - val_loss: 0.2685 - val_f1_score: 0.8197\n",
      "Epoch 6/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0486 - f1_score: 0.9543\n",
      "Epoch 00006: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0486 - f1_score: 0.9543 - val_loss: 0.3051 - val_f1_score: 0.8234\n",
      "Epoch 7/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0404 - f1_score: 0.9636\n",
      "Epoch 00007: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0404 - f1_score: 0.9636 - val_loss: 0.3106 - val_f1_score: 0.8130\n",
      "Epoch 8/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0332 - f1_score: 0.9700\n",
      "Epoch 00008: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0332 - f1_score: 0.9700 - val_loss: 0.3641 - val_f1_score: 0.8264\n",
      "Epoch 9/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0348 - f1_score: 0.9720\n",
      "Epoch 00009: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0348 - f1_score: 0.9720 - val_loss: 0.3968 - val_f1_score: 0.8227\n",
      "Epoch 10/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0285 - f1_score: 0.9766\n",
      "Epoch 00010: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0285 - f1_score: 0.9766 - val_loss: 0.4391 - val_f1_score: 0.8230\n",
      "Epoch 11/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0250 - f1_score: 0.9815\n",
      "Epoch 00011: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0250 - f1_score: 0.9815 - val_loss: 0.4545 - val_f1_score: 0.8208\n",
      "Epoch 12/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0227 - f1_score: 0.9839\n",
      "Epoch 00012: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0227 - f1_score: 0.9839 - val_loss: 0.4596 - val_f1_score: 0.8098\n",
      "Epoch 13/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0160 - f1_score: 0.9864\n",
      "Epoch 00013: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0160 - f1_score: 0.9864 - val_loss: 0.4696 - val_f1_score: 0.8108\n",
      "Epoch 14/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0200 - f1_score: 0.9865\n",
      "Epoch 00014: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0200 - f1_score: 0.9865 - val_loss: 0.4623 - val_f1_score: 0.8138\n",
      "Epoch 15/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0157 - f1_score: 0.9855\n",
      "Epoch 00015: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0157 - f1_score: 0.9855 - val_loss: 0.4864 - val_f1_score: 0.8249\n",
      "Epoch 16/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0133 - f1_score: 0.9894\n",
      "Epoch 00016: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 169ms/step - loss: 0.0133 - f1_score: 0.9894 - val_loss: 0.5219 - val_f1_score: 0.8232\n",
      "Epoch 17/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0140 - f1_score: 0.9896\n",
      "Epoch 00017: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 167ms/step - loss: 0.0140 - f1_score: 0.9896 - val_loss: 0.5464 - val_f1_score: 0.8176\n",
      "Epoch 18/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0159 - f1_score: 0.9853\n",
      "Epoch 00018: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 7s 192ms/step - loss: 0.0159 - f1_score: 0.9853 - val_loss: 0.5428 - val_f1_score: 0.8236\n",
      "Epoch 19/30\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.0138 - f1_score: 0.9872\n",
      "Epoch 00019: val_f1_score did not improve from 0.83365\n",
      "37/37 [==============================] - 6s 171ms/step - loss: 0.0138 - f1_score: 0.9872 - val_loss: 0.5886 - val_f1_score: 0.8039\n",
      "Epoch 20/30\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.0130 - f1_score: 0.9868"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(max_len,))\n",
    "    x = Embedding(len(embed_mat), feat_vec, weights=[np.array(embed_mat)], input_length=max_len,trainable=True)(inp) # trọng số của từ sẽ không được train lại\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Conv1D(64,3,activation=\"relu\")(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "#     convo1 = Conv1D(64,3,activation=\"relu\")(x)\n",
    "#     convo1_maxpool = GlobalMaxPool1D()(convo1)\n",
    "#     convo2 = Conv1D(128,4,activation=\"relu\")(x)\n",
    "#     convo2_maxpool = GlobalMaxPool1D()(convo2)\n",
    "#     convo3 = Conv1D(64,5,activation=\"relu\")(x)\n",
    "#     convo3_maxpool = GlobalMaxPool1D()(convo3)\n",
    "#     concat1 = Concatenate()([convo1_maxpool, convo2_maxpool])\n",
    "#     concat2 = Concatenate()([concat1, convo3_maxpool])\n",
    "#     x = Dense(256, activation=\"relu\")(concat2)\n",
    "    \n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "    \n",
    "# model = get_model(X_train, X_test, y_train, y_test)\n",
    "# model = get_model(padded_doc_shopee, X_test_tiki, output_train_shopee, y_test_tiki)\n",
    "model = get_model()\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.005), metrics= [f1])\n",
    "model.fit(X_train_shopee, y_train_shopee, batch_size=64, epochs=30, validation_data=(X_test_shopee, y_test_shopee), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
    "weights = model.get_weights()\n",
    "\n",
    "for name, weight in zip(names, weights):\n",
    "    print(name, weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 57)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 57, 300)           1571100   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 57, 256)           439296    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 55, 64)            49216     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 2,077,794\n",
      "Trainable params: 2,077,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "new_model = get_model()\n",
    "new_model.load_weights('./weights/best_teacher.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5237, 300)\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].name\n",
    "weight_1=new_model.layers[1].get_weights()\n",
    "print(weight_1[0].shape)\n",
    "# with open('./weights/embedding_trained_mat.txt', 'w') as f:\n",
    "#     f.write(json.dumps(weight_1[0].tolist()))\n",
    "np.save('./weights/embedding_trained_mat', weight_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_2 = new_model.layers[2].get_weights()\n",
    "len(weight_2[0])\n",
    "# with open('./weights/lstm_trained_mat.txt', 'w') as f:\n",
    "#     f.write(json.dumps(np.array(weight_2).tolist()))\n",
    "np.save('./weights/lstm_trained_mat', weight_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_3 = new_model.layers[3].get_weights()\n",
    "len(weight_2[0])\n",
    "# with open('./weights/lstm_trained_mat.txt', 'w') as f:\n",
    "#     f.write(json.dumps(np.array(weight_2).tolist()))\n",
    "np.save('./weights/convo_trained_mat', weight_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def evaluate(X_test, y_test, model):    \n",
    "    y_pre = model.predict(X_test)\n",
    "    for thresh in np.arange(0.2,0.7,0.01):\n",
    "        print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(y_test,(y_pre>thresh).astype(int), average='macro')))\n",
    "    return y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold 0.20 f1 score:0.804\n",
      "threshold 0.21 f1 score:0.807\n",
      "threshold 0.22 f1 score:0.807\n",
      "threshold 0.23 f1 score:0.809\n",
      "threshold 0.24 f1 score:0.809\n",
      "threshold 0.25 f1 score:0.812\n",
      "threshold 0.26 f1 score:0.812\n",
      "threshold 0.27 f1 score:0.812\n",
      "threshold 0.28 f1 score:0.814\n",
      "threshold 0.29 f1 score:0.815\n",
      "threshold 0.30 f1 score:0.814\n",
      "threshold 0.31 f1 score:0.815\n",
      "threshold 0.32 f1 score:0.814\n",
      "threshold 0.33 f1 score:0.816\n",
      "threshold 0.34 f1 score:0.814\n",
      "threshold 0.35 f1 score:0.816\n",
      "threshold 0.36 f1 score:0.816\n",
      "threshold 0.37 f1 score:0.818\n",
      "threshold 0.38 f1 score:0.818\n",
      "threshold 0.39 f1 score:0.819\n",
      "threshold 0.40 f1 score:0.819\n",
      "threshold 0.41 f1 score:0.819\n",
      "threshold 0.42 f1 score:0.819\n",
      "threshold 0.43 f1 score:0.818\n",
      "threshold 0.44 f1 score:0.817\n",
      "threshold 0.45 f1 score:0.817\n",
      "threshold 0.46 f1 score:0.817\n",
      "threshold 0.47 f1 score:0.816\n",
      "threshold 0.48 f1 score:0.815\n",
      "threshold 0.49 f1 score:0.815\n",
      "threshold 0.50 f1 score:0.813\n",
      "threshold 0.51 f1 score:0.812\n",
      "threshold 0.52 f1 score:0.811\n",
      "threshold 0.53 f1 score:0.808\n",
      "threshold 0.54 f1 score:0.808\n",
      "threshold 0.55 f1 score:0.806\n",
      "threshold 0.56 f1 score:0.807\n",
      "threshold 0.57 f1 score:0.807\n",
      "threshold 0.58 f1 score:0.806\n",
      "threshold 0.59 f1 score:0.805\n",
      "threshold 0.60 f1 score:0.805\n",
      "threshold 0.61 f1 score:0.803\n",
      "threshold 0.62 f1 score:0.804\n",
      "threshold 0.63 f1 score:0.802\n",
      "threshold 0.64 f1 score:0.803\n",
      "threshold 0.65 f1 score:0.804\n",
      "threshold 0.66 f1 score:0.804\n",
      "threshold 0.67 f1 score:0.804\n",
      "threshold 0.68 f1 score:0.804\n",
      "threshold 0.69 f1 score:0.803\n"
     ]
    }
   ],
   "source": [
    "y_pre=evaluate(X_test_shopee, y_test_shopee, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh=0.41\n",
    "y_pre_round = []\n",
    "for i in range(len(y_pre)):\n",
    "    y_pre_round.append(1*(y_pre[i] >= thresh))\n",
    "    y_pre_round[i] = y_pre_round[i].tolist()\n",
    "    if 1 not in y_pre_round[i]:\n",
    "        y_pre_round[i].append(1)\n",
    "    else: y_pre_round[i].append(0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\acer\\Documents\\hoc tap\\khóa luận\\mebe_domain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: [0.9458762886597938, 0.9021739130434783, 0.75, 0.8043478260869565, 0.7638888888888888, 0.8764044943820225, 0.3048780487804878]\n",
      "r: [0.9458762886597938, 0.9431818181818182, 0.72, 0.7254901960784313, 0.6626506024096386, 0.8041237113402062, 0.5952380952380952]\n",
      "f1: [0.9458762886597938, 0.9222222222222224, 0.7346938775510204, 0.7628865979381443, 0.7096774193548386, 0.8387096774193549, 0.40322580645161293]\n",
      "micro: (0.875139353400223, 0.839572192513369, 0.8569868995633189)\n",
      "macro: (0.84044856851019, 0.8002204361116481, 0.8190110138575624)\n"
     ]
    }
   ],
   "source": [
    "from modules.evaluate import cal_aspect_prf\n",
    "\n",
    "X = cal_aspect_prf(y_test_ori_shopee.tolist(), y_pre_round, num_of_aspect=7, verbal=True)\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = ['ship','giá','chính hãng''','chất lượng','dịch vụ','an toàn','others']\n",
    "# res = {'label':label,'p':X[0], 'r':X[1], 'f1':X[2]}\n",
    "# df = pd.DataFrame(res)\n",
    "# df1 = pd.DataFrame({'micro':X[3], 'macro': X[4]})\n",
    "# final_df=pd.concat([df,df1], axis=1)\n",
    "\n",
    "# final_df.to_csv(r\"C:\\Users\\acer\\Documents\\hoc tap\\khóa luận\\mebe_domain_pre\\ket qua\\baseline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
