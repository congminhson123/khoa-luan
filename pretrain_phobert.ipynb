{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a081d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from vncorenlp import VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "046608cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import operator\n",
    "from models import Input, AspectOutput\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import json\n",
    "\n",
    "\n",
    "def load_aspect_data_mebe(path):\n",
    "    \"\"\"\n",
    "\n",
    "    :param path:\n",
    "    :return:\n",
    "    :rtype: (list of models.Input, list of models.AspectOutput)\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    df = pd.read_csv(path, encoding='utf-8')\n",
    "    for _, r in df.iterrows():\n",
    "        t = str(r['text']).strip()\n",
    "        inputs.append(t)\n",
    "        labels = list(range(10))\n",
    "        scores = [0 if r['aspect{}'.format(i)] == 0 else 1 for i in range(1, 8)]\n",
    "        outputs.append(scores)\n",
    "    outputs = np.array(outputs)\n",
    "    return inputs, outputs\n",
    "inputs_shopee, outputs_shopee = load_aspect_data_mebe('data/raw_data/mebe_shopee.csv')\n",
    "inputs_tiki, outputs_tiki = load_aspect_data_mebe('data/raw_data/mebe_tiki.csv')\n",
    "inputs = inputs_shopee+inputs_tiki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb61f943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6004"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b18512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41b1bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call for PhoBERT pretrained model from huggingface-transformers\n",
    "phoBert = AutoModel.from_pretrained('vinai/phobert-base', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base', use_fast=False)\n",
    "\n",
    "# Load and use RDRSegmenter from VnCoreNLP as recommended by PhoBERT authors\n",
    "# rdrsegmenter = VnCoreNLP(r\"C:\\Users\\acer\\Documents\\hoc tap\\KT LAB\\scikit-learn\\VnCoreNLP-master\\VnCoreNLP-1.1.1.jar\",\n",
    "#                          annotators='wseg', max_heap_size='-Xmx500m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f6084c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mua đóng_gói cẩn_thận hình_ảnh t / c kiếm xu'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76e198ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = rdrsegmenter.tokenize(inputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1690b5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mua đóng_gói cẩn_thận hình_ảnh t / c kiếm xu']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "for x in x:\n",
    "        a.append(' '.join(x))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81867b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmented_texts = []  # Texts which are segmented\n",
    "# # for inp in inputs[:10]:\n",
    "# for inp in inputs:\n",
    "#     _inp = rdrsegmenter.tokenize(inp.text)\n",
    "#     for sen in _inp:\n",
    "#         segmented_texts.append(' '.join(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bbec315",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_texts = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc3827e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = []  # Texts are converted to indices vectors with 2 added token indices for <s>, </s>\n",
    "for st in segmented_texts:\n",
    "    _st = tokenizer.encode(st)\n",
    "    encoded_texts.append(_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eee4019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a269c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 0\n",
    "for i in encoded_texts:\n",
    "    if len(i) > maxlen: maxlen = len(i)\n",
    "maxlen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5f84293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 188, 8179, 3289, 425, 1204, 95, 1894, 1095, 7044, 2]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f06af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_pos = []\n",
    "for mp in encoded_texts:\n",
    "    m = [int(token_id > 0) for token_id in mp]\n",
    "    masked_pos.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b2a30bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors, masks = [], [] # Convert list of indices to torch tensor\n",
    "for i in range(len(masked_pos)):\n",
    "    tensors.append(torch.tensor([encoded_texts[i]]))\n",
    "    masks.append(torch.tensor([masked_pos[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dc490f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = []  # There are 13 tensors of 13 attention layers from PhoBERT <=> 1 word has 13 (768,)-tensor\n",
    "for i in range(len(tensors)):\n",
    "    with torch.no_grad():\n",
    "        f = phoBert(tensors[i], masks[i])\n",
    "        hs = f[2]  # Len: 13 as 13 output tensors from 13 attention layers\n",
    "        _hs = np.squeeze(np.array([x.detach().numpy() for x in hs]), axis=1)    # Reduce the dimension\n",
    "        lhs.append(_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8940d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lhs[50].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "860cd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_lhs = []   # Shape: num_words * 13 * 768\n",
    "for rlhs in lhs:\n",
    "    _rlhs = []\n",
    "    for i in range(rlhs.shape[1]):\n",
    "        a = np.array([x[i] for x in rlhs])\n",
    "        _rlhs.append(a)\n",
    "    reshaped_lhs.append(_rlhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33162f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(reshaped_lhs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "240346fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_token_emb = []\n",
    "for tte in reshaped_lhs:\n",
    "    _tte = []\n",
    "    for i in tte:\n",
    "        emb = tf.reduce_sum(i[-4:], axis=0)\n",
    "        _tte.append(emb)\n",
    "    texts_token_emb.append(np.array(_tte[1:-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c074d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('phobert_embed', texts_token_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3481918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04f5afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert_embed = np.load('phobert_embed.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba4463f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert_embed[40].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9f3cb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.0757074 ,  2.2905777 , -0.9053175 , ..., -0.04593234,\n",
       "        -2.077567  ,  1.2567792 ],\n",
       "       [-1.5911925 ,  2.013134  , -0.57891965, ...,  0.49071312,\n",
       "        -0.953002  , -0.48284584],\n",
       "       [-0.0750351 ,  2.0042949 ,  0.21631894, ..., -0.25082076,\n",
       "        -0.511614  ,  0.7755858 ],\n",
       "       ...,\n",
       "       [-0.7758073 ,  0.42336088, -0.21910533, ...,  0.24243325,\n",
       "        -0.41877595,  1.0976824 ],\n",
       "       [-0.36669153,  0.6132056 , -0.02825534, ...,  0.16419297,\n",
       "        -1.3461001 ,  0.4586584 ],\n",
       "       [-1.7559828 ,  1.8148768 ,  0.595081  , ..., -0.89722955,\n",
       "        -0.6953317 ,  0.30204165]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_token_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12e982c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=[1,2,3,4]\n",
    "s[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd0f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "maxlen=0\n",
    "for i in phobert_embed:\n",
    "    if len(i) > maxlen: maxlen = len(i)\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfff5896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phobert_embed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408fcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
